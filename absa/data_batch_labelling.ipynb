{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e56f00d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import io \n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from typing import List, Literal\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d2cdc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3050ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"grecotel_data_labelling_key\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7eed511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define valid aspects and sentiments\n",
    "VALID_ASPECTS = [\n",
    "    \"ROOM\", \"FOOD\", \"SERVICE\", \"FACILITIES\", \n",
    "    \"LOCATION\", \"VALUE_FOR_MONEY\", \"CLEANLINESS\", \"COMFORT\"\n",
    "]\n",
    "VALID_SENTIMENTS = [\"positive\", \"negative\"]\n",
    "\n",
    "# Define Pydantic Models for Validation ---\n",
    "\n",
    "class AspectSentimentPair(BaseModel):\n",
    "    \"\"\"Represents a single aspect and its sentiment found in a sentence.\"\"\"\n",
    "    model_config = ConfigDict(extra='forbid')\n",
    "    aspect: Literal[\n",
    "        \"ROOM\", \"FOOD\", \"SERVICE\", \"FACILITIES\", \n",
    "        \"LOCATION\", \"VALUE_FOR_MONEY\", \"CLEANLINESS\", \"COMFORT\"\n",
    "    ]\n",
    "    sentiment: Literal[\"positive\", \"negative\"]\n",
    "\n",
    "class SentenceAnalysis(BaseModel):\n",
    "    \"\"\"Represents the complete analysis of a single sentence.\"\"\"\n",
    "    model_config = ConfigDict(extra='forbid')\n",
    "    results: List[AspectSentimentPair] = Field(\n",
    "        description=\"A list of aspect-sentiment pairs identified in the sentence. Empty if no relevant aspects are found.\"\n",
    "    )\n",
    "\n",
    "# Get the JSON schema for use in the Batch API request body\n",
    "RESPONSE_SCHEMA = SentenceAnalysis.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcd3b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the System Prompt as a constant for use in the Batch API request body\n",
    "SYSTEM_PROMPT = f\"\"\"\n",
    "You are an expert in Aspect-Based Sentiment Analysis (ABSA) for hotel reviews.\n",
    "Your task is to extract Aspect Categories and their Sentiments from the review text.\n",
    "\n",
    "Allowed Aspect Categories: {\", \".join(VALID_ASPECTS)}\n",
    "Allowed Sentiments: {\", \".join(VALID_SENTIMENTS)}\n",
    "\n",
    "Rules:\n",
    "- If a sentence mentions multiple aspects, extract all of them.\n",
    "- If a sentence has no relevant aspects from the list, return an empty list.\n",
    "- Be precise. Implicit aspects are CRITICAL. You must infer the aspect from context.\n",
    "\n",
    "Output Format: JSON object with a \"results\" key containing a list of aspect-sentiment pairs.\n",
    "\n",
    "--- EXAMPLES ---\n",
    "\n",
    "Example 1 (Simple):\n",
    "Input: \"The breakfast was delicious and fresh.\"\n",
    "Output: {{ \"results\": [ {{ \"aspect\": \"FOOD\", \"sentiment\": \"positive\" }} ] }}\n",
    "\n",
    "Example 2 (Mixed Sentiment & Multiple Aspects):\n",
    "Input: \"Great location near the metro, but the bathroom was dirty and the staff was unhelpful.\"\n",
    "Output: {{ \"results\": [ \n",
    "    {{ \"aspect\": \"LOCATION\", \"sentiment\": \"positive\" }},\n",
    "    {{ \"aspect\": \"CLEANLINESS\", \"sentiment\": \"negative\" }},\n",
    "    {{ \"aspect\": \"SERVICE\", \"sentiment\": \"negative\" }}\n",
    "] }}\n",
    "\n",
    "Example 3 (Implicit Aspects):\n",
    "Input: \"The walls are paper thin, I could hear the neighbors talking all night. Also, it costs too much for what you get.\"\n",
    "Output: {{ \"results\": [ \n",
    "    {{ \"aspect\": \"COMFORT\", \"sentiment\": \"negative\" }},\n",
    "    {{ \"aspect\": \"VALUE_FOR_MONEY\", \"sentiment\": \"negative\" }}\n",
    "] }}\n",
    "\n",
    "Example 4 (No relevant info):\n",
    "Input: \"I traveled with my family in July.\"\n",
    "Output: {{ \"results\": [] }}\n",
    "\n",
    "--- END EXAMPLES ---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d51c39e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_full_dataset_batch(df: pd.DataFrame, sample_size: int = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes a DataFrame of reviews using the OpenAI Batch API.\n",
    "\n",
    "    Handles file creation, upload, batch submission, status checking,\n",
    "    result retrieval, and final parsing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Filter and Sample Data\n",
    "    df = df[df['lang'] == 'en'].copy()\n",
    "    if sample_size:\n",
    "        df = df.sample(n=sample_size)\n",
    "    \n",
    "    print(f\"Preparing {len(df)} reviews for batch processing...\")\n",
    "    \n",
    "    input_records = []\n",
    "    \n",
    "    # 2. Prepare JSONL Input File Content\n",
    "    # The batch API requires a JSONL file where each line is an API request body.\n",
    "    for index, row in df.iterrows():\n",
    "        custom_id = f\"review-{row['id']}-{index}\"\n",
    "        \n",
    "        # Structure the request body for /v1/chat/completions\n",
    "        request_body = {\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"Analyze the review: {row['review_text']}\"}\n",
    "            ],\n",
    "            \"response_format\": {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"review_analysis\",\n",
    "                    \"strict\": True,\n",
    "                    \"schema\": RESPONSE_SCHEMA\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Structure the top-level batch request object\n",
    "        input_records.append({\n",
    "            \"custom_id\": custom_id,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": request_body\n",
    "        })\n",
    "\n",
    "    # 3. Create JSONL file\n",
    "    input_file_content = \"\\n\".join([json.dumps(record) for record in input_records])\n",
    "    input_filename = \"batch_input.jsonl\"\n",
    "    with open(input_filename, \"w\") as f:\n",
    "        f.write(input_file_content)\n",
    "\n",
    "    # 4. Upload Input File\n",
    "    print(\"Uploading input file...\")\n",
    "    input_file = client.files.create(file=open(input_filename, \"rb\"), purpose=\"batch\")\n",
    "    \n",
    "    # 5. Create and Submit Batch Job\n",
    "    print(\"Submitting batch job...\")\n",
    "    batch_job = client.batches.create(\n",
    "        input_file_id=input_file.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\"\n",
    "    )\n",
    "    print(f\"Batch Job Started! ID: {batch_job.id}\")\n",
    "    \n",
    "    # 6. Check Job Status\n",
    "    print(\"Waiting for batch job to complete...\")\n",
    "    while batch_job.status not in [\"completed\", \"failed\", \"cancelled\"]:\n",
    "        time.sleep(10)\n",
    "        batch_job = client.batches.retrieve(batch_job.id)\n",
    "        print(f\"Current status: {batch_job.status} | Processed: {batch_job.request_counts.completed} | Failed: {batch_job.request_counts.failed}\")\n",
    "\n",
    "    if batch_job.status != \"completed\":\n",
    "        print(f\"Batch job ended with status: {batch_job.status}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # 7. Download and Parse Results\n",
    "    # Check if we have an output file (successes/failures handled gracefully) or error file\n",
    "    result_file_id = batch_job.output_file_id\n",
    "    if not result_file_id:\n",
    "        print(\"No output file generated. Checking for error file...\")\n",
    "        result_file_id = batch_job.error_file_id\n",
    "        \n",
    "    if not result_file_id:\n",
    "        print(\"CRITICAL: No output or error file found. The batch may have failed completely.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Downloading results from file ID: {result_file_id}...\")\n",
    "    output_content = client.files.content(result_file_id).content.decode('utf-8')\n",
    "    \n",
    "    labeled_data = []\n",
    "    \n",
    "    # Parse the JSONL output file\n",
    "    for line in output_content.splitlines():\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        result = json.loads(line)\n",
    "        custom_id = result.get('custom_id')\n",
    "        \n",
    "        # Check if the individual request failed (non-200 status)\n",
    "        response_data = result.get('response', {})\n",
    "        if response_data.get('status_code') != 200:\n",
    "            print(f\"Request failed for {custom_id}: {response_data}\")\n",
    "            continue\n",
    "\n",
    "        # Match back to original dataframe\n",
    "        original_review_row = df.loc[df.index[df.apply(lambda r: f\"review-{r['id']}-{r.name}\" == custom_id, axis=1)]]\n",
    "        if original_review_row.empty: continue\n",
    "\n",
    "        r_id = original_review_row['id'].iloc[0]\n",
    "        text = original_review_row['review_text'].iloc[0]\n",
    "\n",
    "        try:\n",
    "            # Extract content\n",
    "            analysis_results = response_data['body']['choices'][0]['message']['content']\n",
    "            parsed_analysis = json.loads(analysis_results)\n",
    "            \n",
    "            aspect_sentiment_pairs = parsed_analysis.get('results', [])\n",
    "            \n",
    "            if aspect_sentiment_pairs:\n",
    "                for item in aspect_sentiment_pairs:\n",
    "                    labeled_data.append({\n",
    "                        \"id\": r_id,\n",
    "                        \"review_text\": text, \n",
    "                        \"aspect\": item['aspect'],\n",
    "                        \"sentiment\": item['sentiment']\n",
    "                    })\n",
    "            else:\n",
    "                labeled_data.append({\"id\": r_id, \"review_text\": text, \"aspect\": None, \"sentiment\": None})\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing result for {custom_id}: {e}\")\n",
    "              \n",
    "    return pd.DataFrame(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bfd4d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_dataset_in_safe_chunks(df: pd.DataFrame, chunk_size: int = 2500):\n",
    "    \"\"\"\n",
    "    Splits the dataset into smaller chunks to stay under the 2M token limit.\n",
    "    Processes each chunk sequentially (Submit -> Wait -> Download -> Next).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter for English\n",
    "    df = df[df['lang'] == 'en'].copy()\n",
    "    \n",
    "    # --- CORRECTED SPLITTING LOGIC ---\n",
    "    # Use list slicing to create a list of DataFrames. \n",
    "    # This guarantees chunk_df is a pandas DataFrame.\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "    num_chunks = len(chunks)\n",
    "    \n",
    "    print(f\"Dataset too large for Tier 1 limit. Split into {num_chunks} chunks of max {chunk_size} reviews.\")\n",
    "    \n",
    "    all_labeled_data = []\n",
    "    \n",
    "    for i, chunk_df in enumerate(chunks):\n",
    "        print(f\"\\n--- Processing Chunk {i+1}/{num_chunks} ({len(chunk_df)} reviews) ---\")\n",
    "        \n",
    "        # 1. Prepare JSONL\n",
    "        input_records = []\n",
    "        # Now chunk_df is definitely a DataFrame, so .iterrows() works safely\n",
    "        for index, row in chunk_df.iterrows():\n",
    "            custom_id = f\"review-{row['id']}-{index}\"\n",
    "            request_body = {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": f\"Analyze the review: {row['review_text']}\"}\n",
    "                ],\n",
    "                \"response_format\": {\n",
    "                    \"type\": \"json_schema\", \n",
    "                    \"json_schema\": {\n",
    "                        \"name\": \"review_analysis\",\n",
    "                        \"strict\": True,\n",
    "                        \"schema\": RESPONSE_SCHEMA\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            input_records.append({\n",
    "                \"custom_id\": custom_id,\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": request_body\n",
    "            })\n",
    "\n",
    "        input_filename = f\"batch_input_chunk_{i+1}.jsonl\"\n",
    "        with open(input_filename, \"w\") as f:\n",
    "            f.write(\"\\n\".join([json.dumps(r) for r in input_records]))\n",
    "\n",
    "        # 2. Upload & Submit\n",
    "        print(f\"Uploading chunk {i+1}...\")\n",
    "        input_file = client.files.create(file=open(input_filename, \"rb\"), purpose=\"batch\")\n",
    "        \n",
    "        print(f\"Submitting chunk {i+1}...\")\n",
    "        batch_job = client.batches.create(\n",
    "            input_file_id=input_file.id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "        )\n",
    "        print(f\"Chunk {i+1} Started! Job ID: {batch_job.id}\")\n",
    "        \n",
    "        # 3. Wait Loop (BLOCKING)\n",
    "        # We MUST wait for this to finish before submitting the next one\n",
    "        while batch_job.status not in [\"completed\", \"failed\", \"cancelled\"]:\n",
    "            time.sleep(30) # Check every 30s\n",
    "            batch_job = client.batches.retrieve(batch_job.id)\n",
    "            print(f\"Chunk {i+1} Status: {batch_job.status}...\")\n",
    "\n",
    "        if batch_job.status != \"completed\":\n",
    "            print(f\"Chunk {i+1} Failed! Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # 4. Download Results\n",
    "        print(f\"Downloading results for Chunk {i+1}...\")\n",
    "        result_file_id = batch_job.output_file_id\n",
    "        if result_file_id:\n",
    "            output_content = client.files.content(result_file_id).content.decode('utf-8')\n",
    "            \n",
    "            # Parse results\n",
    "            for line in output_content.splitlines():\n",
    "                if not line: continue\n",
    "                result = json.loads(line)\n",
    "                custom_id = result.get('custom_id')\n",
    "                \n",
    "                response_data = result.get('response', {})\n",
    "                if response_data.get('status_code') == 200:\n",
    "                    try:\n",
    "                        analysis_results = response_data['body']['choices'][0]['message']['content']\n",
    "                        parsed_analysis = json.loads(analysis_results)\n",
    "                        \n",
    "                        # Find original text. The custom_id is \"review-{id}-{index}\"\n",
    "                        # We extract the 'id' part to look it up.\n",
    "                        orig_id_str = custom_id.split('-')[1]\n",
    "                        orig_id = int(orig_id_str)\n",
    "                        \n",
    "                        # We look up the text in the specific chunk_df to be safe and fast\n",
    "                        text_match = chunk_df[chunk_df['id'] == orig_id]\n",
    "                        if not text_match.empty:\n",
    "                            text = text_match.iloc[0]['review_text']\n",
    "                            \n",
    "                            for item in parsed_analysis.get('results', []):\n",
    "                                all_labeled_data.append({\n",
    "                                    \"id\": orig_id,\n",
    "                                    \"review_text\": text,\n",
    "                                    \"aspect\": item['aspect'],\n",
    "                                    \"sentiment\": item['sentiment']\n",
    "                                })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing item in chunk {i+1}: {e}\")\n",
    "\n",
    "        # Cleanup for this chunk\n",
    "        try:\n",
    "            client.files.delete(input_file.id)\n",
    "            if batch_job.output_file_id: client.files.delete(batch_job.output_file_id)\n",
    "            os.remove(input_filename)\n",
    "        except Exception as e:\n",
    "            print(f\"Cleanup warning: {e}\")\n",
    "            \n",
    "    return pd.DataFrame(all_labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b2ed477",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.join(parent_dir, 'data','cleaned','cleaned_dataset_tripadvisor-reviews_2025-11-01_14-21-09-431.json')\n",
    "\n",
    "df = pd.read_json(data_path, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "835177ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11989, 8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['lang'] == 'en'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79253dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing 5 reviews for batch processing...\n",
      "Uploading input file...\n",
      "Submitting batch job...\n",
      "Batch Job Started! ID: batch_693da8821f7c819092c617e8523d859a\n",
      "Waiting for batch job to complete...\n",
      "Current status: in_progress | Processed: 0 | Failed: 0\n",
      "Current status: in_progress | Processed: 2 | Failed: 0\n",
      "Current status: in_progress | Processed: 2 | Failed: 0\n",
      "Current status: in_progress | Processed: 2 | Failed: 0\n",
      "Current status: in_progress | Processed: 2 | Failed: 0\n",
      "Current status: in_progress | Processed: 2 | Failed: 0\n",
      "Current status: in_progress | Processed: 2 | Failed: 0\n",
      "Current status: in_progress | Processed: 2 | Failed: 0\n",
      "Current status: in_progress | Processed: 2 | Failed: 0\n",
      "Current status: in_progress | Processed: 2 | Failed: 0\n",
      "Current status: in_progress | Processed: 2 | Failed: 0\n",
      "Current status: in_progress | Processed: 2 | Failed: 0\n",
      "Current status: in_progress | Processed: 4 | Failed: 0\n",
      "Current status: finalizing | Processed: 5 | Failed: 0\n",
      "Current status: completed | Processed: 5 | Failed: 0\n",
      "Downloading results from file ID: file-MYo1LSFVDwkjx62qTiK1br...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review_text</th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>609653550</td>\n",
       "      <td>Terrible smell. I am writing this review from ...</td>\n",
       "      <td>CLEANLINESS</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>609653550</td>\n",
       "      <td>Terrible smell. I am writing this review from ...</td>\n",
       "      <td>COMFORT</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>609653550</td>\n",
       "      <td>Terrible smell. I am writing this review from ...</td>\n",
       "      <td>VALUE_FOR_MONEY</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1015557492</td>\n",
       "      <td>Relaxing natural environment &amp; superb staff. W...</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1015557492</td>\n",
       "      <td>Relaxing natural environment &amp; superb staff. W...</td>\n",
       "      <td>SERVICE</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                        review_text  \\\n",
       "0   609653550  Terrible smell. I am writing this review from ...   \n",
       "1   609653550  Terrible smell. I am writing this review from ...   \n",
       "2   609653550  Terrible smell. I am writing this review from ...   \n",
       "3  1015557492  Relaxing natural environment & superb staff. W...   \n",
       "4  1015557492  Relaxing natural environment & superb staff. W...   \n",
       "\n",
       "            aspect sentiment  \n",
       "0      CLEANLINESS  negative  \n",
       "1          COMFORT  negative  \n",
       "2  VALUE_FOR_MONEY  negative  \n",
       "3         LOCATION  positive  \n",
       "4          SERVICE  positive  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_df_sample = process_full_dataset_batch(df, sample_size=5)\n",
    "labeled_df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a061c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset too large for Tier 1 limit. Split into 5 chunks of max 2500 reviews.\n",
      "\n",
      "--- Processing Chunk 1/5 (2500 reviews) ---\n",
      "Uploading chunk 1...\n",
      "Submitting chunk 1...\n",
      "Chunk 1 Started! Job ID: batch_693dafebfa8c8190806a60181126af56\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: in_progress...\n",
      "Chunk 1 Status: finalizing...\n",
      "Chunk 1 Status: finalizing...\n",
      "Chunk 1 Status: finalizing...\n",
      "Chunk 1 Status: finalizing...\n",
      "Chunk 1 Status: finalizing...\n",
      "Chunk 1 Status: finalizing...\n",
      "Chunk 1 Status: finalizing...\n",
      "Chunk 1 Status: finalizing...\n",
      "Chunk 1 Status: finalizing...\n",
      "Chunk 1 Status: finalizing...\n",
      "Chunk 1 Status: finalizing...\n",
      "Chunk 1 Status: finalizing...\n",
      "Chunk 1 Status: completed...\n",
      "Downloading results for Chunk 1...\n",
      "\n",
      "--- Processing Chunk 2/5 (2500 reviews) ---\n",
      "Uploading chunk 2...\n",
      "Submitting chunk 2...\n",
      "Chunk 2 Started! Job ID: batch_693db5cd20e4819099e2dc97e8206ffb\n",
      "Chunk 2 Status: in_progress...\n",
      "Chunk 2 Status: in_progress...\n",
      "Chunk 2 Status: in_progress...\n",
      "Chunk 2 Status: in_progress...\n",
      "Chunk 2 Status: in_progress...\n",
      "Chunk 2 Status: in_progress...\n",
      "Chunk 2 Status: in_progress...\n",
      "Chunk 2 Status: in_progress...\n",
      "Chunk 2 Status: in_progress...\n",
      "Chunk 2 Status: in_progress...\n",
      "Chunk 2 Status: in_progress...\n",
      "Chunk 2 Status: in_progress...\n",
      "Chunk 2 Status: finalizing...\n",
      "Chunk 2 Status: finalizing...\n",
      "Chunk 2 Status: finalizing...\n",
      "Chunk 2 Status: finalizing...\n",
      "Chunk 2 Status: finalizing...\n",
      "Chunk 2 Status: finalizing...\n",
      "Chunk 2 Status: finalizing...\n",
      "Chunk 2 Status: finalizing...\n",
      "Chunk 2 Status: finalizing...\n",
      "Chunk 2 Status: finalizing...\n",
      "Chunk 2 Status: finalizing...\n",
      "Chunk 2 Status: completed...\n",
      "Downloading results for Chunk 2...\n",
      "\n",
      "--- Processing Chunk 3/5 (2500 reviews) ---\n",
      "Uploading chunk 3...\n",
      "Submitting chunk 3...\n",
      "Chunk 3 Started! Job ID: batch_693db8b822048190aecedcb8df417785\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: in_progress...\n",
      "Chunk 3 Status: finalizing...\n",
      "Chunk 3 Status: finalizing...\n",
      "Chunk 3 Status: finalizing...\n",
      "Chunk 3 Status: finalizing...\n",
      "Chunk 3 Status: finalizing...\n",
      "Chunk 3 Status: finalizing...\n",
      "Chunk 3 Status: finalizing...\n",
      "Chunk 3 Status: finalizing...\n",
      "Chunk 3 Status: finalizing...\n",
      "Chunk 3 Status: finalizing...\n",
      "Chunk 3 Status: finalizing...\n",
      "Chunk 3 Status: completed...\n",
      "Downloading results for Chunk 3...\n",
      "\n",
      "--- Processing Chunk 4/5 (2500 reviews) ---\n",
      "Uploading chunk 4...\n",
      "Submitting chunk 4...\n",
      "Chunk 4 Started! Job ID: batch_693dc5867e908190bb9a4fa939113df4\n",
      "Chunk 4 Status: validating...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: in_progress...\n",
      "Chunk 4 Status: finalizing...\n",
      "Chunk 4 Status: finalizing...\n",
      "Chunk 4 Status: finalizing...\n",
      "Chunk 4 Status: finalizing...\n",
      "Chunk 4 Status: finalizing...\n",
      "Chunk 4 Status: completed...\n",
      "Downloading results for Chunk 4...\n",
      "\n",
      "--- Processing Chunk 5/5 (1989 reviews) ---\n",
      "Uploading chunk 5...\n",
      "Submitting chunk 5...\n",
      "Chunk 5 Started! Job ID: batch_693dc97f71708190a9d9838d90328986\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: in_progress...\n",
      "Chunk 5 Status: finalizing...\n",
      "Chunk 5 Status: finalizing...\n",
      "Chunk 5 Status: finalizing...\n",
      "Chunk 5 Status: completed...\n",
      "Downloading results for Chunk 5...\n",
      "Done! Saved to labeled_hotel_reviews_full.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_df = process_dataset_in_safe_chunks(df, chunk_size=2500)\n",
    "final_df.to_json(\"labeled_hotel_reviews_full.json\", orient='records', indent=2)\n",
    "print(\"Done! Saved to labeled_hotel_reviews_full.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee2761db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11961"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d42c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
